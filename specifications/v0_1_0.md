# Reactive Agents Standard

**Version:** 0.1.0

**Status:** Draft

**Date:** 2025-11-18

**Authors:** Reactive Agents Contributors

## Abstract

This document defines a standard for **Reactive Agents**: AI systems that continuously optimize their behavior through experimentation, evaluation, and feedback. A reactive agent decomposes tasks into skills, maintains multiple configuration variants (arms) for each context cluster, and uses online learning algorithms to select and evolve high-performing configurations over time. This standard enables implementations to provide transparent self-optimization while maintaining compatibility with existing AI provider APIs.

## 1. Introduction

### 1.1 Motivation

Current AI systems require manual prompt engineering, hyperparameter tuning, and model selection. This process is time-consuming, difficult to optimize across diverse use cases, and doesn't adapt to changing requirements or user feedback. Reactive Agents address these challenges by:

- **Automating optimization**: Continuously experimenting with configurations to improve performance
- **Contextual adaptation**: Clustering requests by semantic similarity and optimizing per-cluster
- **Transparent operation**: Working behind existing API interfaces without client-side changes
- **Data-driven evolution**: Using real feedback and evaluations to guide improvements

### 1.2 Scope

This standard defines:

- **Core architecture**: The hierarchical structure of agents, skills, clusters, and arms
- **Optimization interface**: How systems select and update variants based on performance
- **Evaluation system**: How performance is measured and feedback is provided
- **API compatibility**: How reactive agents integrate with existing AI provider interfaces
- **Observability requirements**: What must be logged for auditability and reproducibility

This standard does **not** prescribe:

- Specific optimization algorithms (though reference algorithms are recommended)
- Particular evaluation metrics (only the interface for providing feedback)
- Internal implementation details beyond the required interfaces

### 1.3 Terminology

- **Agent**: A top-level entity that performs decision-making tasks
- **Skill**: A specific capability or sub-task that an agent can perform
- **Cluster**: A semantic grouping of similar requests within a skill
- **Arm**: A specific configuration variant (e.g., system prompt, model parameters) for handling requests
- **Configuration**: The complete set of parameters that define an arm's behavior
- **Evaluation**: A method for assessing the quality of an agent's response
- **Feedback**: Performance signal used to update arm statistics
- **Optimization**: The process of selecting and evolving arms based on feedback

## 2. Core Architecture

### 2.1 Hierarchical Structure

A reactive agent system MUST implement a four-level hierarchy:

```
Agent
  └── Skill (1..n)
       └── Cluster (1..n)
            └── Arm (2..n)
```

**Agent**: Represents a complete AI system with a defined purpose. An agent contains one or more skills.

**Skill**: Represents a specific capability or task type within an agent. Each skill:
- Has a clear description of what it does
- Operates independently with its own optimization loop
- Contains multiple clusters for contextual adaptation

**Cluster**: Represents a semantic grouping of similar requests within a skill. Clusters enable context-aware optimization by:
- Grouping requests with similar characteristics
- Maintaining separate optimization state per context
- Allowing different arms to succeed in different contexts

**Arm**: Represents a specific configuration variant for handling requests. Each arm:
- Contains a complete configuration (system prompt, model, parameters)
- Maintains performance statistics
- Competes with other arms in the same cluster

### 2.2 Minimum Requirements

**Agents** MUST:
- Have at least one skill to be operational
- Have a unique identifier
- Have a description of their purpose

**Skills** MUST:
- Have at least one cluster
- Have at least one configuration (model) available
- Have a clear description for optimization guidance
- If optimization is enabled, have at least one evaluation method configured

**Clusters** MUST:
- Have at least two arms for A/B testing and optimization
- Maintain a representation of the request space they cover (e.g., centroid vector)
- Track the number of requests processed for optimization triggering

**Arms** MUST:
- Have a complete, executable configuration
- Maintain performance statistics (at minimum: request count, mean reward)
- Be independently evaluable

### 2.3 Configuration Flexibility

Implementations MAY organize the hierarchy differently (e.g., flat structure with just arms, deep hierarchies with additional levels) as long as they satisfy the minimum requirement: **multiple variants (arms) exist and are optimized based on performance feedback**.

## 3. Request Routing and Arm Selection

### 3.1 Request Flow

When a request arrives:

1. **Skill Selection**: Route the request to the appropriate skill based on the request type or routing logic
2. **Cluster Assignment**: Determine which cluster the request belongs to (if using clustering)
3. **Arm Selection**: Use an optimization algorithm to select an arm from the cluster
4. **Execution**: Execute the request using the selected arm's configuration
5. **Logging**: Record the request, response, and metadata for evaluation
6. **Evaluation**: Assess the response quality (immediately or asynchronously)
7. **Update**: Update arm statistics based on evaluation results

### 3.2 Cluster Assignment

If using semantic clustering, implementations SHOULD:

- Generate an embedding vector for each request
- Compare the request embedding to cluster centroids
- Assign the request to the nearest cluster
- Periodically recompute centroids based on recent requests

Implementations MAY use alternative clustering methods or skip clustering entirely (treating each skill as having a single cluster).

### 3.3 Arm Selection

Implementations MUST use an optimization algorithm to select arms that balances:

- **Exploitation**: Selecting high-performing arms
- **Exploration**: Trying under-tested arms to discover better options

Recommended algorithms include:
- **Thompson Sampling**: Bayesian approach sampling from posterior distributions
- **Upper Confidence Bound (UCB)**: Select arms based on mean + confidence interval
- **Epsilon-Greedy**: Select best arm with probability (1-ε), random arm with probability ε

The selection algorithm SHOULD be configurable to control exploration/exploitation tradeoff.

## 4. Evaluation and Feedback

### 4.1 Evaluation Interface

Implementations MUST provide an interface for evaluating agent responses. The evaluation interface MUST support:

- **Multiple evaluation methods**: Different criteria for assessing quality
- **Weighted aggregation**: Combining multiple evaluation scores with configurable weights
- **Asynchronous evaluation**: Evaluation may happen after the response is returned
- **Sparse feedback**: Not every request requires evaluation

### 4.2 Evaluation Methods

An evaluation method takes:
- **Input**: The request and response
- **Output**: A score normalized to [0, 1] where 1 is best

Common evaluation method types include:
- **Rule-based**: Checking for specific patterns, formats, or content
- **Model-based**: Using an AI model to judge quality
- **Human feedback**: Collecting ratings from users
- **Downstream metrics**: Success in subsequent tasks or user actions

Implementations SHOULD support pluggable evaluation methods, allowing users to define custom evaluators.

### 4.3 Feedback Requirements

The feedback system MUST:
- Accept sparse feedback (some requests may never receive feedback)
- Handle delayed feedback (evaluation may occur long after the request)
- Support multiple simultaneous evaluation methods per skill
- Normalize all scores to [0, 1] for consistent aggregation

### 4.4 Statistical Updates

When feedback is received, implementations MUST update arm statistics. At minimum, track:
- **n**: Number of times the arm was played
- **mean**: Average reward (evaluation score)
- **total_reward**: Sum of all rewards

Implementations MAY track additional statistics (variance, confidence intervals, etc.) to improve selection algorithms.

## 5. Configuration Evolution

### 5.1 Evolution Interface

Implementations MUST provide a mechanism for configurations to evolve over time. The evolution interface requires:

- **Trigger conditions**: When to generate new configurations
- **Update mechanism**: How to modify or replace existing arms
- **State reset**: When to reset statistics after configuration changes

### 5.2 Evolution Patterns

Implementations SHOULD NOT prescribe specific evolution mechanisms but MAY include:

- **Manual updates**: Users modify configurations directly
- **Automated generation**: AI-generated variations of high-performing configurations
- **Reflection-based**: Analyzing successes and failures to create targeted improvements
- **Parameter tuning**: Adjusting numerical parameters within defined ranges

### 5.3 Conservative Updates

When evolving configurations, implementations SHOULD follow conservative update strategies:

- **Preserve best**: Never modify the current best-performing arm
- **Test gradually**: Introduce new configurations alongside existing ones
- **Reset statistics**: Reset statistics for modified arms to ensure fair comparison
- **Maintain history**: Keep records of past configurations for rollback

## 6. API Compatibility

### 6.1 Provider API Compatibility

Implementations SHOULD support compatibility with existing AI provider APIs to enable transparent operation. This means:

- Accepting requests in standard formats (e.g., OpenAI chat completions API)
- Returning responses in standard formats
- Supporting standard parameters (model, temperature, etc.)
- Adding reactive agent functionality transparently

### 6.2 Request Format

Implementations SHOULD accept standard API requests and MAY extend them with reactive agent parameters:

```json
{
  "messages": [...],
  "model": "gpt-5",

  "reactive_agents": {
    "agent": "customer-support",
    "skill": "answer-question",
    "optimization": {
      "type": "semantic",
      "enabled": true
    }
  }
}
```

### 6.3 Response Format

Implementations MUST return standard API responses. Implementations MAY include metadata about the reactive agent process:

```json
{
  "id": "chatcmpl-...",
  "object": "chat.completion",
  "choices": [...],

  "reactive_agents": {
    "agent_id": "uuid",
    "skill_id": "uuid",
    "cluster_id": "uuid",
    "arm_id": "uuid",
    "log_id": "uuid"
  }
}
```

## 7. Observability Requirements

### 7.1 Required Logging

Implementations MUST log the following for each request:

**Request Information**:
- Unique request ID
- Timestamp (start, end, first token if streaming)
- Request method and endpoint
- Request body (or sanitized version)

**Routing Information**:
- Agent, skill, cluster, and arm IDs used
- Selection algorithm state (e.g., samples, scores)

**Response Information**:
- Response status code
- Response body (or sanitized version)
- Execution duration

**Evaluation Information**:
- Evaluation method used
- Raw scores from each evaluation method
- Aggregated score
- Timestamp of evaluation

### 7.2 Optional Logging

Implementations SHOULD log:

**Embeddings**:
- Request embedding vector for clustering
- Cluster centroid for similarity calculation

**Tracing Information**:
- Trace ID, span ID, parent span ID
- Integration with distributed tracing systems

**User Metadata**:
- Application ID
- External user ID
- Custom metadata fields

### 7.3 Retention and Privacy

Implementations MUST:
- Support configurable retention policies for logs
- Provide mechanisms for sanitizing sensitive data
- Enable GDPR-compliant data deletion
- Support audit trails for configuration changes

## 8. Security Considerations

### 8.1 Authentication

Implementations MUST:
- Require authentication for API access
- Support standard authentication methods (API keys, OAuth, etc.)
- Isolate agent data between different users/tenants

### 8.2 Data Protection

Implementations SHOULD:
- Encrypt sensitive data at rest and in transit
- Provide mechanisms for PII redaction in logs
- Support role-based access control (RBAC)
- Enable audit logging for administrative actions

### 8.3 Model Safety

Implementations SHOULD:
- Validate generated configurations before deployment
- Provide mechanisms for human review of evolved configurations
- Support rollback to previous configurations
- Monitor for harmful or unexpected behavior

## 9. Reference Implementation

### 9.1 Architecture Overview

The reference implementation (github.com/idkhub-com/reactive-agents) demonstrates:

- **Thompson Sampling**: Beta distribution-based arm selection with temperature control
- **Semantic Clustering**: Embedding-based request clustering with cosine similarity
- **Two-Phase Evolution**:
  - Early regeneration after 5 requests with real examples
  - Ongoing reflection-based refinement per cluster
- **OpenAI-Compatible API**: Drop-in replacement for OpenAI endpoints

### 9.2 Key Design Decisions

**Thompson Sampling with Temperature**:
```
alpha = (successes + 1 - 1) / temperature + 1
beta = (failures + 1 - 1) / temperature + 1
sample = Beta(alpha, beta)
```
- Temperature = 1.0: Standard Thompson Sampling
- Temperature > 1.0: More exploration
- Temperature < 1.0: More exploitation

**Conservative Reflection**:
- Best arm is never modified (guaranteed no regression)
- Worst arm gets best arm's config + new prompt (stats reset)
- Middle arms get new prompt only (stats reset)
- Cluster total_steps set to best arm's n (maintains valid history)

**Weighted Evaluations**:
- Multiple evaluation methods per skill
- Configurable weights for each method
- Per-evaluation statistics tracked separately
- Weighted average used for arm selection

## 10. Compliance and Conformance

### 10.1 Conformance Levels

**Level 1 (Minimal Reactive Agent)**:
- Multiple arms per skill
- Basic arm selection (epsilon-greedy or random)
- Feedback interface for updating statistics
- Request logging

**Level 2 (Standard Reactive Agent)**:
- Clustering or contextual routing
- Thompson Sampling or UCB algorithm
- Multiple evaluation methods with weighting
- Configuration evolution mechanism
- API compatibility layer

**Level 3 (Advanced Reactive Agent)**:
- Semantic clustering with embeddings
- Temperature-controlled Thompson Sampling
- Automated configuration generation
- Distributed tracing integration
- Real-time analytics

### 10.2 Interoperability

Implementations MAY provide export/import mechanisms for:
- Agent definitions
- Skill configurations
- Evaluation methods
- Historical logs and statistics

This enables portability between reactive agent implementations.

## 11. Future Directions

Potential extensions to this standard:

**Planned for Version 1.1**:
- **Model training and fine-tuning**: Using collected logs and evaluations to train or fine-tune models, not just optimize prompts and parameters

**Under Consideration**:
- **Multi-agent orchestration**: Standards for coordinating multiple reactive agents
- **Transfer learning**: Sharing learned optimizations across agents or deployments
- **Federated learning**: Privacy-preserving optimization across multiple deployments
- **Causal inference**: Understanding why certain configurations perform better
- **Active learning**: Strategically requesting feedback for maximum learning

## 12. References

- Multi-Armed Bandit Algorithms: Lattimore & Szepesvári, "Bandit Algorithms" (2020)
- Thompson Sampling: Thompson, "On the Likelihood that One Unknown Probability Exceeds Another" (1933)
- Contextual Bandits: Agarwal et al., "Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits" (2014)
- OpenAI API Specification: https://platform.openai.com/docs/api-reference
