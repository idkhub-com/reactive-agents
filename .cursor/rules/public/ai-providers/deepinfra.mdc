---
description: 
globs: lib/server/ai-providers/deepinfra/*
alwaysApply: false
---
## Instructions for AI Agent

When working with the DeepInfra AI provider directory (`lib/server/ai-providers/deepinfra/`), you MUST:

### 1. Check Official Documentation First
- Always reference the official DeepInfra API documentation at: https://deepinfra.com/docs
- Verify current API endpoints, parameters, and response formats
- Check for any recent API changes, deprecations, or new model additions
- Review DeepInfra's model catalog and serverless capabilities

### 2. Validate API Endpoint Coverage
- Ensure all implemented endpoints match the official DeepInfra API specification
- Verify HTTP methods, request/response schemas, and parameter requirements
- Check that error handling follows DeepInfra's documented error responses (400, 401, 403, 429, 500)
- Implement proper handling for DeepInfra-specific error codes and rate limiting

### 3. Review Implementation Patterns
- Confirm request/response type definitions align with DeepInfra's official schemas
- Validate authentication mechanisms (Authorization header with Bearer token)
- Ensure rate limiting and retry logic follows DeepInfra's guidelines
- Implement proper timeout handling and connection management
- Handle DeepInfra's serverless cold start considerations

### 4. Security Best Practices
- Never log or expose API keys in plaintext
- Use environment variables or secure storage for API key management
- Implement proper input validation and sanitization
- Follow DeepInfra's usage policies and content guidelines
- Validate all user inputs before sending to the API
- Implement proper error handling to avoid exposing sensitive information

### 5. Check for Missing Features
- Compare implemented functionality against the full DeepInfra API surface:
  - Chat Completions API
  - Completions API
  - Embeddings API
  - Image generation models
  - Audio processing models
  - Streaming responses
  - Batch processing (if available)
- Verify streaming implementation matches DeepInfra's Server-Sent Events specification
- Check support for latest open-source models in their catalog

### 6. Validate Configuration
- Ensure model names match DeepInfra's current model catalog
- Check default values align with DeepInfra's documented defaults
- Validate token limits and context windows for each model
- Implement proper model availability and capability checks
- Handle DeepInfra-specific parameters correctly
- Account for serverless cold start times in timeout configurations

## Provider-Specific Considerations
- **Serverless Infrastructure**: Optimize for DeepInfra's serverless deployment model
- **Open Source Models**: Keep updated with latest open-source models (Llama, Mistral, etc.)
- **Cold Start Handling**: Implement proper handling of serverless cold starts
- **Model Variety**: Support diverse model types (text, image, audio, embedding)
- **Cost Efficiency**: Optimize for pay-per-use serverless pricing model
- **Performance**: Balance between cost and performance for serverless deployments

## Before Making Changes to DeepInfra Provider
- Consult https://deepinfra.com/docs for the latest specification
- Cross-reference with existing implementation in `lib/server/ai-providers/deepinfra/`
- Test with DeepInfra's platform or official examples
- Ensure changes maintain backward compatibility where possible
- Update type definitions to match any API schema changes
- Review DeepInfra's model catalog for latest additions

## Files to Review
- All files under `lib/server/ai-providers/deepinfra/`
- Related type definitions in `lib/shared/types/`
- DeepInfra-specific configurations, constants, and model definitions
- Authentication and error handling utilities
- Serverless optimization and cold start handling implementations

**Note:** This rule ensures the DeepInfra provider implementation stays current with the official API specification, follows security best practices, and properly leverages DeepInfra's serverless AI inference capabilities and diverse model catalog.
