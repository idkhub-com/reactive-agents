---
description: 
globs: lib/server/ai-providers/predibase/*
alwaysApply: false
---
## Instructions for AI Agent

When working with the Predibase AI provider directory (`lib/server/ai-providers/predibase/`), you MUST:

### 1. Check Official Documentation First
- Always reference the official Predibase API documentation at: https://docs.predibase.com/
- Verify current API endpoints, parameters, and response formats
- Check for any recent API changes, deprecations, or new model releases
- Review Predibase's fine-tuning and inference capabilities

### 2. Validate API Endpoint Coverage
- Ensure all implemented endpoints match the official Predibase API specification
- Verify HTTP methods, request/response schemas, and parameter requirements
- Check that error handling follows Predibase's documented error responses (400, 401, 403, 429, 500)
- Implement proper handling for Predibase-specific error codes and rate limiting

### 3. Review Implementation Patterns
- Confirm request/response type definitions align with Predibase's official schemas
- Validate authentication mechanisms (Authorization header with Bearer token)
- Ensure rate limiting and retry logic follows Predibase's guidelines
- Implement proper timeout handling and connection management
- Handle Predibase's specific request/response formats correctly

### 4. Security Best Practices
- Never log or expose API keys in plaintext
- Use environment variables or secure storage for API key management
- Implement proper input validation and sanitization
- Follow Predibase's usage policies and content guidelines
- Validate all user inputs before sending to the API
- Implement proper error handling to avoid exposing sensitive information

### 5. Check for Missing Features
- Compare implemented functionality against the full Predibase API surface:
  - Chat Completions API
  - Completions API
  - Fine-tuning API
  - Model deployment and management
  - Embeddings API (if available)
  - Streaming responses
  - Custom model inference
- Verify streaming implementation matches Predibase's Server-Sent Events specification
- Check support for base models and fine-tuned models in their catalog

### 6. Validate Configuration
- Ensure model names match Predibase's current model catalog
- Check default values align with Predibase's documented defaults
- Validate token limits and context windows for each model
- Implement proper model availability and capability checks
- Handle Predibase-specific parameters correctly
- Support fine-tuned model configurations

## Provider-Specific Considerations
- **Fine-Tuning Focus**: Leverage Predibase's strength in model fine-tuning and customization
- **Model Management**: Handle both base models and fine-tuned model deployments
- **Performance**: Optimize for Predibase's inference performance characteristics
- **Custom Models**: Support custom fine-tuned models and their specific configurations
- **Cost Efficiency**: Consider fine-tuning costs vs. inference costs optimization
- **Model Lifecycle**: Handle model training, deployment, and versioning workflows

## Before Making Changes to Predibase Provider
- Consult https://docs.predibase.com/ for the latest specification
- Cross-reference with existing implementation in `lib/server/ai-providers/predibase/`
- Test with Predibase's platform or official examples
- Ensure changes maintain backward compatibility where possible
- Update type definitions to match any API schema changes
- Review Predibase's model catalog and fine-tuning capabilities

## Files to Review
- All files under `lib/server/ai-providers/predibase/`
- Related type definitions in `lib/shared/types/`
- Predibase-specific configurations, constants, and model definitions
- Authentication and error handling utilities
- Fine-tuning and model management implementations

**Note:** This rule ensures the Predibase provider implementation stays current with the official API specification, follows security best practices, and properly leverages Predibase's fine-tuning and custom model inference capabilities.
